---
title: "Multiple Linear Regression (Model Selection and Model Checking) - Lab"
author: "Blake Pappas"
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Savings Rates in 50 Countries

The savings data frame has 50 rows (countries) and 5 columns (variables):

1. \texttt{sr}: savings rate - personal saving divided by disposable income
*This variable will be used as the response*
2. \texttt{pop15}: percent population under age of 15
3. \texttt{pop75}: percent population over age of 75
4. \texttt{dpi}: per-capita disposable income in dollars
5. \texttt{ddpi}: percent growth rate of dpi

The data is averaged over the period 1960-1970. 



*Data Source:* Belsley, D., Kuh. E. and Welsch, R. (1980) *Regression Diagnostics* Wiley.


Load the dataset.

**Code:**

```{r}
data(savings, package = "faraway")
head(savings)
```

1. Perform the best subset selection and select the "best" model using $R^{2}_{adj}$.

**Code:**

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(caret)
library(leaps)
models <- regsubsets(sr ~ ., data = savings) # regsubsets = the function for model selection
summary(models) # Gives best model based on the number of predictors


res.sum <- summary(models)

criteria <- data.frame(
  Adj.R2 = res.sum$adjr2,
  Cp = res.sum$cp,
  BIC = res.sum$bic)

criteria

# Plot of Adjusted R-Squared
plot(2:5, criteria$Adj.R2, las = 1, xlab = "p", ylab = "", pch = 16, col = "gray",
     main = expression(R['adj']^2))
points(4, criteria$Adj.R2[3], col = "blue", pch = 16)
```

**Answer: The best model using $R^{2}_{adj}$ is the third model, which uses the `pop15`, `pop75`, and `ddpi` as the predictors.**


2. Perform a stepwise selection using *AIC*.

**Code:**

```{r}
full <- lm(sr ~ ., data = savings)
step(full, direction = "both")
```

**Answer: Using the stepwise selection, we are brought to an AIC of 136.45, which uses `pop15`, `pop75`, and `ddpi` as the predictors for the regression having the best goodness of fit.**

3. Perform a general linear F-test (with $\alpha = 0.1$) to choose between the full model (i.e., using the all 4 predictors) and the reduced model that includes `pop15`, `pop75`, and `ddpi` as the predictors.

**Code:**

```{r}
# Reduced Model
reduce <- lm(sr ~ pop15 + pop75 + ddpi, data = savings)
summary(reduce)

# Full Model
full <- lm(sr ~ ., data = savings)
summary(full)

# General Linear F-Test
anova(reduce, full)
```

**Answer: Since the p-value of this general linear F-test is greater than Î±, we fail to reject H0 and conclude that we do not have sufficient
evidence to support that at least one of the three regression coefficients is not equal to 0.**

4. Make a residual plot of the model selected by *AIC* and comment on the model assumptions.

**Code:**

```{r}
mod <- lm(formula = sr ~ pop15 + pop75 + ddpi, data = savings)
plot(mod$fitted.values, mod$residuals, pch = 16, col = "blue", xlab = "Fitted Values", ylab = "Residuals", main = "Residuals vs. Fitted")
abline(h = 0, col = "red")
```

**Answer: There is no major concern with the model assumptions, as the plot of the residuals appears to be random.**

5. Use both a histogram and qqplot to examine the normality assumption on error.

**Code:**

```{r}
# Histogram
(sd <- sd(mod$residuals))

par(las = 1)
hist(mod$residuals, 5, prob = T, col = "lightblue", border = "gray", main = "Histogram of Residuals", xlab = "Residuals")
xg <- seq(-10, 10, 1)
yg <- dnorm(xg, 0, sd)
lines(xg, yg)

# qqplot
qqnorm(mod$residuals, pch = 16, las = 1, col = "gray", xlab = "Normal Quantiles", ylab = "Residuals")
qqline(mod$residuals)
```

**Answer: There is no major concern regarding normality with this model. The distribution of the residuals appear to be approximately normally distributed. The Normal Q-Q Plot appears to run closely (in an S-shaped pattern) to the trend line,with little deviation.**

6. Calculate the leverage values to check if there is any high leverage points (i.e., $h > \frac{2p}{n}$).

**Code:**

```{r}
step_savings <- step(full, trace = F) # Trace = full
X <- model.matrix(step_savings) # Model Design Matrix
H <- X %*% solve((t(X) %*% X)) %*% t(X)
lev <- hat(X) # Calculates leverage
high_lev <- which(lev >= 2 * 3 / 30) # Finds the high leverage values
attach(savings)

# Plot of Leverage Points
par(las = 1)
plot(pop15, dpi, cex = sqrt(5 * lev), col = "blue", ylim = c(0, 5000))
points(pop15[high_lev], dpi[high_lev], col = "red", pch = 16,
       cex = sqrt(5 *lev[high_lev]))
```

**Answer: There are two high leverage points between `pop15` and `dpi`. They exist in observations 23 an 49.**

7. Compute jackknife residuals to identify outlier(s).

**Code:**

```{r}
jack <- rstudent(step_savings)

par(las = 1)
plot(jack, pch = 16, cex = 0.8, col = "blue", main = "Jackknife Residuals",
     xlab = "", ylab = "")
abline(h = 0, lty = 2, col = "gray")
```

**Answer: Looking at the graph, there do not appear to be many outliers.**

8. Identifying influential observations by computing DFFITS.

**Code:**

```{r, message = FALSE, warning = FALSE}
library(olsrr)
ols_plot_dffits(step_savings)
```

**Answer: There are three influential observations for `sr`: Observations 23, 46, and 49.**