---
title: "Ensemble Learning and Text Mining"
author: "Blake Pappas"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
bookmarks: no
---
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning = FALSE, message = FALSE}
library(dplyr)
library(caret)
library(rpart)
library(e1071)
library(fastAdaboost)
library(ipred)
library(randomForest)
library(tm)
library(wordcloud)
```

# Part I. Ensemble Learning in R

# Load the "bank_small.csv" data file:

```{r, warning = FALSE}
bank = read.csv("bank_small.csv", stringsAsFactors = TRUE)

train_rows = createDataPartition(y = bank$y,
                                 p = 0.70, list = FALSE)

bank_train = bank[train_rows, ]
bank_test = bank[-train_rows, ]
```


## Bagging Using Training/Testing Split

```{r, warning = FALSE}
bag_model = bagging(y ~ ., data = bank_train, nbagg = 50)

# Make Predictions and Evaluate Performance
pred = predict(bag_model, bank_test)
confusionMatrix(pred, bank_test$y, mode = "prec_recall", positive = "yes")
```


## Bagging Using Cross Validation and For Loop

```{r, warning = FALSE}
# Create the Folds
cv = createFolds(bank$y, k = 5)

# Make a Vector to Store Model Type, Number of Bagging Models, and F1 from Each Fold
bagging_models = vector()
F_Measure = vector()

for(i in seq(50, 250, 50)) {
  F1 = vector()
  
  # Loop Through Each Fold
  for(test_rows in cv) {
    bank_train = bank[-test_rows, ]
    bank_test = bank[test_rows, ]
    
    # Train the Model and Evaluate Performance
    bag_model = bagging(y ~ ., data = bank_train, nbagg = i)
    pred = predict(bag_model, bank_test)
    cm = confusionMatrix(pred, bank_test$y, mode = "prec_recall", positive = "yes")
    
    # Add the F1 of the Current Fold
    F1 = append(F1, cm$byClass[7])
  }
  bagging_models = append(bagging_models, i)
  F_Measure = append(F_Measure, mean(F1))
}

data.frame(Bagging_Models = bagging_models, F_Measure = F_Measure)
```


## Boosting Using Training/Testing Split

```{r, warning = FALSE}
adaboost_model = adaboost(y ~ ., data = bank_train, nIter = 100)

# Make Predictions and Evaluate Performance
pred = predict(adaboost_model, bank_test)
confusionMatrix(pred$class, bank_test$y, mode = "prec_recall", positive = "yes")
```


## Boosting Using Cross Validation and For Loop

```{r, warning = FALSE}
# Create the Folds
cv = createFolds(bank$y, k = 5)

# Make a Vector to Store Model Type, Number of Iterations, and F1 from Each Fold
iterations = vector()
F_Measure = vector()

for(i in seq(50, 250, 50)) {
  F1 = vector()
  
  # Loop Through Each Fold
  for(test_rows in cv) {
    bank_train = bank[-test_rows, ]
    bank_test = bank[test_rows, ]
    
    # Train the Model and Evaluate Performance
    adaboost_model = adaboost(y ~ ., data = bank_train, nIter = i)
    pred = predict(adaboost_model, bank_test)
    cm = confusionMatrix(pred$class, bank_test$y, mode = "prec_recall", positive = "yes")
    
    # Add the F1 of the Current Fold
    F1 = append(F1, cm$byClass[7])
  }
  iterations = append(iterations, i)
  F_Measure = append(F_Measure, mean(F1))
}

data.frame(Iterations = iterations, F_Measure = F_Measure)
```


## Random Forest Using Training/Testing Split

```{r, warning = FALSE}
rf_model = randomForest(y ~ ., data = bank_train, ntree = 100)

# Make Predictions and Evaluate Performance
pred = predict(rf_model, bank_test)
confusionMatrix(pred, bank_test$y, mode = "prec_recall", positive = "yes")
```


## Random Forest Using Cross Validation and For Loop

```{r, warning = FALSE}
# Create the Folds
cv = createFolds(bank$y, k = 5)

# Make a Vector to Store Model Type, Number of Iterations, and F1 from Each Fold
trees = vector()
F_Measure = vector()

for(i in seq(50, 250, 50)) {
  F1 = vector()
  
  # Loop Through Each Fold
  for(test_rows in cv) {
    bank_train = bank[-test_rows, ]
    bank_test = bank[test_rows, ]
    
    # Train the Model and Evaluate Performance
    rf_model = randomForest(y ~ ., data = bank_train, ntree = i)
    pred = predict(rf_model, bank_test)
    cm = confusionMatrix(pred, bank_test$y, mode = "prec_recall", positive = "yes")
    
    # Add the F1 of the Current Fold
    F1 = append(F1, cm$byClass[7])
  }
  trees = append(trees, i)
  F_Measure = append(F_Measure, mean(F1))
}

data.frame(Trees = trees, F_Measure = F_Measure)
```


## Stacking Using Training/Testing Split

```{r, warning = FALSE}
# Build a Stacking Model

train_rows = createDataPartition(y = bank$y,
                                 p = 0.50, list = FALSE)

bank_train = bank[train_rows, ]
bank_test = bank[-train_rows, ]


# Train Three Base Learners
tree_model = rpart(y ~ ., data = bank_train, method = "class",
                   parms = list(split = "information"))

logit_model = glm(y ~ ., data = bank_train,
                  family = binomial(link = "logit"))

svm_model = svm(y ~ ., data = bank_train,
                kernel = "polynomial", degree = 2)

# Make Predictions Using Base Learners
pred_tree = predict(tree_model, bank_test, type = "class")

pred_logit_prob = predict(logit_model, bank_test, type = "response")

pred_logit = ifelse(pred_logit_prob > 0.5, "yes", "no")

pred_svm = predict(svm_model, bank_test)

# Add Base Learners' Predictions to the bank_test Data
bank_test = bank_test %>%
  mutate(pred_tree = pred_tree,
         pred_logit = pred_logit,
         pred_svm = pred_svm)

# Do the Second Split
train2_rows = createDataPartition(y = bank_test$y,
                                  p = 0.50, list = FALSE)

bank_train2 = bank_test[train2_rows, ]
bank_test2 = bank_test[-train2_rows, ]

# Build the Naive Bayes Combiner
nb_model = naiveBayes(y ~ ., data = bank_train2)

# Evaluate the Naive Bayes Model
pred_nb = predict(nb_model, bank_test2)

confusionMatrix(factor(pred_nb), bank_test2$y,
                mode = "prec_recall", positive = "yes")
```


# Part II. Text Mining in R

# 1. Import the "FB Posts.csv" data file into R. Then, convert the text into a corpus.

```{r, warning = FALSE}
fb_posts_text = read.csv("FB Posts.csv")

fb_posts_corpus = Corpus(VectorSource(fb_posts_text$Text))
```


# 2. Print out the contents of the 100th post

```{r, warning = FALSE}
fb_posts_corpus[[100]]$content
```


# 3. Remove all punctuations from the posts

```{r, warning = FALSE}
fb_posts_corpus = tm_map(fb_posts_corpus, removePunctuation)
```


# 4. Convert all text to lowercase

```{r, warning = FALSE}
fb_posts_corpus = tm_map(fb_posts_corpus, tolower)
```


# 5. Remove English stopwords

```{r, warning = FALSE}
fb_posts_corpus = tm_map(fb_posts_corpus, removeWords,
                          stopwords('english'))
```


# 6. Perform Stemming

```{r, warning = FALSE}
fb_posts_corpus = tm_map(fb_posts_corpus, stemDocument)
```


# 7. Obtain the TF-IDF matrix of the corpus

```{r, warning = FALSE}
dtm = DocumentTermMatrix(fb_posts_corpus)

dtm_matrix = as.matrix(dtm)
```


# 8. Report the top 10 most common words in the corpus

```{r, warning = FALSE}
word_freq = colSums(as.matrix(dtm))

word_freq_sorted = sort(word_freq, decreasing = TRUE)

word_freq_sorted[1:10]
```


# 9. Make a WordCloud. Set the minimum word frequency to be 5.

```{r, warning = FALSE}
wordcloud(words = names(word_freq_sorted),
          freq = word_freq_sorted,
          min.freq = 5)
```