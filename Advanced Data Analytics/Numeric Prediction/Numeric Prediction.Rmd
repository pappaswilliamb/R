---
title: "Numeric Prediction"
author: "Blake Pappas"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
bookmarks: no
---
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Numeric Prediction in R
# Load the following packages:

```{r, warning = FALSE, message = FALSE}
library(caret)
library(class)
library(dplyr)
library(glmnet)
```

# In this exercise, we use the "laptop.csv" file. The goal is to predict the "Price" of a laptop based on its attributes.

# P1: Import the dataset. Split it to 80% training and 20% testing.

```{r}
laptop = read.csv("laptop.csv")

train_rows = createDataPartition(y = laptop$Price,
                                 p = 0.80, list = FALSE)

# Training Dataset
laptop_train = laptop[train_rows, ]

# Testing Dataset
laptop_test = laptop[-train_rows, ]
```

# P2: Build a K-NN Model
# Do you need to normalize the data?
# Answer: Yes, the data needs to be normalized.
```{r}
normalize = function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
  }

laptop_normalized = laptop %>% mutate_at(2:8, normalize)
laptop_normalized_train = laptop_normalized[train_rows, ]
laptop_normalized_test = laptop_normalized[-train_rows, ]
```

# Examine the normalized dataset. What went wrong? Why?
# Answer: Because the only value for the field "Screen.Size" is 15, there is a divide by zero error and all the normalized values resulted in a value of NaN.

# Write the code below to fix it:
```{r}
# Don't include the "Screen.Size" field in the normalization function.
laptop_normalized = laptop %>% mutate_at(3:8, normalize)
laptop_normalized_train = laptop_normalized[train_rows, ]
laptop_normalized_test = laptop_normalized[-train_rows, ]
```

# P3: Build a K-NN classifier with a k value of 50
```{r}
pred_knn = knnregTrain(train = laptop_normalized_train[, 3:8],
                       test = laptop_normalized_test[, 3:8],
                       y = laptop_normalized_train[, 1],
                       k = 50)
```

# P4: Evaluate the performance of the K-NN model
```{r}
actual = laptop_normalized_test$Price
error = pred_knn - actual

# Average Error
KNN_AE = mean(error)
KNN_AE

# Mean Absolute Error
KNN_MAE = mean(abs(error))
KNN_MAE

# Mean Absolute Percentage Error
KNN_MAPE = mean(abs(error / actual))
KNN_MAPE

# Root Mean Squared Error
KNN_RMSE = sqrt(mean(error^2))
KNN_RMSE

# Total Sum of Squared Error
KNN_SSE = sum(error^2)
KNN_SSE
```

# P5: Now, repeat P3 and P4 with different k values (e.g., 50-60). Use a loop. Report the RMSE.
```{r}
cv = createFolds(y = laptop$Price, k = 60)

for (test_row in cv) {
  
  laptop_normalized = laptop %>% mutate_at(3:8, normalize)
  laptop_train = laptop_normalized[-test_row, ]
  laptop_test = laptop_normalized[test_row, ]
  
  pred_knn = knnregTrain(train = laptop_train[, 3:8],
                         test = laptop_test[, 3:8],
                         y = laptop_train[, 1],
                         k = 60)
  
  # RMSE
  rmse = sqrt(mean((pred_knn - laptop_test[, 1])^2))
  
  print(rmse)
}
```


# P6: Build a linear regression model. Use cross-validation. Report the mean RMSE.
```{r, warning = FALSE}
cv = createFolds(y = laptop$Price, k = 5)

rmse_cv = c()

for (test_row in cv) {
  
  laptop_train = laptop[-test_row, ]
  laptop_test = laptop[test_row, ]
  
  lm_model = lm(Price ~ ., data = laptop_train)
  
  pred_lm = predict(lm_model, laptop_test)
  
  rmse = sqrt(mean((pred_lm - laptop_test[, 1])^2))
  
  rmse_cv = c(rmse_cv, rmse)
}

# Mean RMSE
print(mean(rmse_cv))
```