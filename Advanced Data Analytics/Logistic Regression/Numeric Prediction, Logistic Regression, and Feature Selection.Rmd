---
title: "Numeric Prediction, Logistic Regression, and Feature Selection"
author: "Blake Pappas"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
bookmarks: no
---
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part I. Numeric Prediction in R

```{r, warning = FALSE, message = FALSE}
library(caret)
library(rpart)
library(rpart.plot)
library(glmnet)
library(dplyr)
```

# Import "purchase.csv" into R:

```{r}
purchase = read.csv("purchase.csv")
```

# 1. Use 5-fold cross-validation to evaluate the performance of the regression tree model on this dataset. Report the average (1) MAE; (2) MAPE; (3) RMSE. 
# Then, build a single regression tree model on the entire dataset, plot the tree, and answer the following two questions.
# (a) How many decision nodes are there in the tree?
# (b) Pick one decision rule from the tree and interpret it.

```{r}
# Cross-Validation
cv = createFolds(y = purchase$Spending, k = 5)

tree_mae_cv = c()

tree_mape_cv = c()

tree_rmse_cv = c()

for (test_row in cv) {
  
  purchase_train = purchase[-test_row, ]
  purchase_test = purchase[test_row, ]
  
  tree = rpart(Spending ~ ., data = purchase_train)
  
  pred_tree = predict(tree, purchase_test)
  
  # MAE
  tree_mae = mean(abs(pred_tree - purchase_test[, 23]))
  mae_cv = c(tree_mae_cv, tree_mae)
  
  # MAPE
  tree_mape = mean(abs((pred_tree - purchase_test[, 23]) / purchase_test[, 23]))
  mape_cv = c(tree_mape_cv, tree_mape)
  
  # RMSE
  tree_rmse = sqrt(mean((pred_tree - purchase_test[, 23])^2))
  
  rmse_cv = c(tree_rmse_cv, tree_rmse)
}

# Average MAE
print(mean(mae_cv))

#Average MAPE
print(mean(mape_cv))

# Average RMSE
print(mean(rmse_cv))


# Build a Single Regression Tree Model
tree = rpart(Spending ~ ., data = purchase_train)

# Plot the Tree
prp(tree, varlen = 0)
```

# Answer: There are __ decision nodes in the tree.
# Answer: 


# 2. Use 5-fold cross-validation to evaluate the performance of the linear regression model on this dataset. Report the average (1) MAE; (2) MAPE; (3) RMSE.
# Then, build a single linear regression model on the entire dataset and examine the model.
# Pick a coefficient and interpret it.

```{r}
# Cross-Validation
cv = createFolds(y = purchase$Spending, k = 5)

lm_mae_cv = c()

lm_mape_cv = c()

lm_rmse_cv = c()

for (test_row in cv) {
  
  purchase_train = purchase[-test_row, ]
  purchase_test = purchase[test_row, ]
  
  lm_model = lm(Spending ~ ., data = purchase_train)
  
  pred_lm = predict(lm_model, purchase_test)
  
  # MAE
  lm_mae = mean(abs(pred_lm - purchase_test[, 23]))
  mae_cv = c(lm_mae_cv, lm_mae)
  
  # MAPE
  lm_mape = mean(abs((pred_lm - purchase_test[, 23]) / purchase_test[, 23]))
  mape_cv = c(lm_mape_cv, lm_mape)
  
  # RMSE
  lm_rmse = sqrt(mean((pred_lm - purchase_test[, 23])^2))
  
  rmse_cv = c(lm_rmse_cv, lm_rmse)
}

# Average MAE
print(mean(mae_cv))

#Average MAPE
print(mean(mape_cv))

# Average RMSE
print(mean(rmse_cv))


# Build a Single Linear Regression Model
lm_model = lm(Spending ~ ., data = purchase_train)

# Examine the Model
summary(lm_model)
```

# A one-unit increase in X is associated with b-units increase in Prediction Y.


# Part II. Logistic Regression and Feature Selection

```{r, warning = FALSE, message = FALSE}
library(FSelectorRcpp)
library(pROC)
```

# 1. Import the data. Convert the "spam" variable to a factor. 

```{r}
spambase = read.csv("spambase.csv")

spambase$spam = factor(spambase$spam)
```

# 2. Use 5-fold cross-validation to evaluate the performance of the logistic regression model on this dataset. Report the average (1) accuracy; (2) precision, recall, and F-measure of class "spam"; (3) AUC of class "spam".

```{r, warning = FALSE}
cv = createFolds(y = spambase$spam, k = 5)

accuracy = c()

for (test_row in cv) {
  
  spambase_train = spambase[-test_row, ]
  spambase_test = spambase[test_row, ]
  
  logit_model = glm(spam ~ ., data = spambase_train,
                    family = binomial(link = "logit"))
  
  # Log Odds
  pred = predict(logit_model, spambase_test)
  
  # Predicted Probability
  pred_prob = predict(logit_model, spambase_test,
                      type = "response")
  
  # Binary Predictions
  pred_binary = ifelse(pred_prob > 0.5, "yes", "no")
  
  cm = confusionMatrix(factor(pred_binary), factor(ifelse(spambase_test$spam == "1", "yes", "no")),
                  mode = "prec_recall", positive = "yes")
  
  accuracy = c(accuracy, cm$overall[1])
}

# Average Accuracy
print(mean(accuracy))

# Precision
print(cm$byClass[5])

# Recall
print(cm$byClass[6])

# F-Measure
print(cm$byClass[7])

# AUC
roc_logit = roc(response = spambase_test$spam,
                predictor = pred_prob)
plot(roc_logit)
auc(roc_logit)
```


# 3. Perform feature selection using the information gain metric. Build the best logistic regression with the selected features. 
# Note: Use a for loop to try different numbers of features. The "best" model is defined as the one with highest AUC for class "spam".
# Report the features and AUC of the "best" model.

```{r, warning = FALSE}
best_auc = 0
best_model = vector()

for (i in 1:57) {
  IG = information_gain(spam ~ ., data = spambase_train)
  
  topK = cut_attrs(IG, k = i)
  
  train = spambase_train %>% select(topK, spam)
  test = spambase_test %>% select(topK, spam)
  
  logit_model = glm(spam ~ ., data = train, family = binomial(link = "logit"))
  pred_prob = predict(logit_model, test, type = "response")
  auc = auc(ifelse(test$spam == 1, 1, 0), pred_prob)
  if (auc > best_auc){
    best_auc = auc
    best_model = topK
  }
}
  
best_model
best_auc
```

# 4. Perform forward feature selection. Build the best logistic regression with the selected features. 
# The "best" model is defined as the one with highest AUC for class "spam", evaluated using 5-fold cross-validation.
# Report the features and AUC of the "best" model.

# Forward Feature Selection
```{r, warning = FALSE, message = FALSE}
best_auc = 0
selected_features = c()
while (TRUE) {
  feature_to_add = -1
  # The elements of setdiff(x, y) are those elements in x, but not in y
  for (i in setdiff(1:57, selected_features)) {
    train = spambase_train %>% select(selected_features, i, spam)
    test = spambase_test %>% select(selected_features, i, spam)
    spambase_best = spambase %>% select(selected_features, i, spam)
    
    logit_model = glm(spam ~ ., data = train,
                      family = binomial(link = "logit"))
    
    pred_prob = predict(logit_model, test,
                        type = "response")
    
    auc = auc(test$spam, pred_prob)
    
    if (auc > best_auc) {
      best_auc = auc
      feature_to_add = i
    }
  }
  
  if (feature_to_add != -1) {
    selected_features = c(selected_features, feature_to_add)
  }
  else break
}

print(selected_features)
print(best_auc)
```

# Cross-Validation
```{r, warning = FALSE}
cv = createFolds(y = spambase_best$spam, k = 5)

accuracy = c()

for (test_row in cv) {
  
  spambase_best_train = train
  spambase_best_test = test
  
  logit_model = glm(spam ~ ., data = spambase_best_train,
                    family = binomial(link = "logit"))
  
  # Log Odds
  pred = predict(logit_model, spambase_best_test)
  
  # Predicted Probability
  pred_prob = predict(logit_model, spambase_best_test,
                      type = "response")
  
  # Binary Predictions
  pred_binary = ifelse(pred_prob > 0.5, "yes", "no")
  
  cm = confusionMatrix(factor(pred_binary), factor(ifelse(spambase_best_test$spam == "1", "yes", "no")),
                       mode = "prec_recall", positive = "yes")
  
  accuracy = c(accuracy, cm$overall[1])
}

# Confusion Matrix
print(cm)

# AUC
roc_logit = roc(response = spambase_best_test$spam,
                predictor = pred_prob)
plot(roc_logit)
auc(roc_logit)
```

# Performance Evaluation
```{r}
# Confusion Matrix
print(cm)

# AUC
roc_logit = roc(response = spambase_best_test$spam,
                predictor = pred_prob)
plot(roc_logit)
auc(roc_logit)
```