---
title: "Data Manipulation, Association Rule Mining, and Clustering Analysis"
author: "Blake Pappas"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
bookmarks: no
---
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part I. Understand Basic Concepts in Association Rule Mining

**Question 1:** After mining some shopping basket dataset, you find that rule “{butter} → {bread}” has confidence of 70% and support of 5%. If “butter” appears in 200 shopping baskets (i.e., transactions), what is the size of the entire dataset that was mined? Explain.

### Answer: The size of the entire data set that was mined is 2,800.

### Explanation:

Confidence = supp(X → Y) / supp(X)

Confidence = 70% (0.7)

Butter appears in 200 shopping baskets

Supp(X → Y) / 200 = 0.7

Supp(X → Y) = 140 <- Butter and bread appearing in the same shopping baskets


Support = # transactions containing both X and Y / # transactions

Support = 5% (0.05)

140 / # transactions = 0.05; # transactions = 2,800

#

**Question 2:** Consider two association rules mined from some shopping basket dataset: “{cereal} → {milk}” and “{cereal, cookies} → {milk}”. Which of the following is true about the support of these rules? Explain.

a. Support({cereal}→{milk}) $\geq$ Support({cereal, cookies}→{milk})
b. Support({cereal}→{milk}) $\leq$ Support({cereal, cookies}→{milk})
c. Not enough information, i.e., it could be either $\geq$ or $\leq$, depending on the specific dataset.

### Answer: a. Support({cereal} → {milk}) $\geq$ Support({cereal, cookies} → {milk})

### Explanation:

The fraction of transactions containing cereal and milk has to be greater than the fraction of transactions containing cereal, cookies, and milk.

As an example:

4 customers visited a coffee shop one morning, generating a dataset of 4 transactions:

1. {yogurt, cereal, milk}

2. {milk, cereal, cookies}

3. {yogurt, milk, cereal, cookies}

4. {milk, cookies}

Support({cereal} → {milk}) = # transactions containing both cereal and milk / # transactions

3 / 4 = 75%

Support({cereal} → {milk}) = 75%

Support({cereal, cookies} → {milk}) = # transactions containing cereal, cookies, and milk / # transactions

2 / 4 = 50%

Support({cereal, cookies} → {milk}) = 50%

Support({cereal} → {milk}) = 75% $\geq$ Support({cereal, cookies} → {milk}) = 50%

#

**Question 3:** Consider two association rules mined from some shopping basket dataset: “{cereal} → {milk}” and “{cereal} → {milk, yogurt}”. Which of the following is true about the confidence of these rules? Explain.

a. Confidence({cereal} → {milk}) $\geq$ Confidence({cereal} → {milk, yogurt})
b. Confidence({cereal} → {milk}) $\leq$ Confidence({cereal} → {milk, yogurt})
c. Not enough information, i.e., it could be either $\geq$ or $\leq$, depending on the specific dataset.

### Answer: a. Confidence({cereal} → {milk}) $\geq$ Confidence({cereal} → {milk, yogurt})

### Explanation:

The probability of milk appearing in transactions that contain cereal must be greater than the probability of milk and yogurt appearing in the same set of transactions containing cereal.

As an example:

4 customers visited a coffee shop one morning, generating a dataset of 4 transactions:

1. {yogurt, cereal, milk}

2. {milk, cereal, cookies}

3. {yogurt, milk, cereal, cookies}

4. {milk, cookies}

Confidence({cereal} → {milk}) = Supp({cereal} → {milk}) / Supp({cereal})

3 / 3 = 100%

Confidence({cereal} → {milk}) = 100%

Confidence({cereal} → {milk, yogurt}) = Supp({cereal} → {milk, yogurt}) / Supp({cereal})

2 / 3 = 67%

Confidence({cereal} → {milk, yogurt}) = 67%


Confidence({cereal} → {milk}) (100%) $\geq$ Confidence({cereal} → {milk, yogurt}) (67%)

#

# Part II. Understand Basic Concepts in Clustering Analysis

**Question 4:** Consider the following "exam.csv" dataset, which shows the results of 3 exams taken by 4 different students (all scores are on the 0-100 scale). Find which two students are most similar to each other (i.e., have smallest distance between them) according to the following distance metrics: **(i) Euclidean, (ii) Manhattan, (iii) max-coordinate**.

```{r, warning = FALSE, message = FALSE}
library(dplyr)

library(stats)

exam = read.csv("exam.csv")
```

```{r}
# Euclidean
euclidian_distance_matrix = dist(exam[, 2:4], method = "euclidean")

as.matrix(euclidian_distance_matrix)
```

```{r}
# Manhattan
manhattan_distance_matrix = dist(exam[, 2:4], method = "manhattan")

as.matrix(manhattan_distance_matrix)
```

```{r}
# Max-Coordinate
max_distance_matrix = dist(exam[, 2:4], method = "maximum")

as.matrix(max_distance_matrix)
```

### Answers:
### Euclidian: K and L are most similar to each other.
### Manhattan: M and N are most similar to each other.
### Max-Coordinate: L and M are most similar to each other.

#

**Question 5:** Consider the following three item lists, which reflect purchases from a small grocery store. Calculate the **Jaccard distance** between each pair of baskets – i.e., calculate *J*(*A*, *B*), *J*(*B*, *C*) and *J*(*A*, *C*). Note: You may find the distances easier to compute if you first convert the item lists into a binary matrix format.

• Basket A = {Pudding, Jam, Salsa}

• Basket B = {Salsa, Pudding, Chips}

• Basket C = {Jam, Chips, Salsa, Pudding}

###

Answer the following two sub-questions:

**Question 5.1:** Which two baskets are farthest apart based on Jaccard distance?

Jaccard Distance : *d*(*A*, *B*) = N~01~ + N~10~ / N~01~ + N~10~ + N~11~

*J*(*A*, *B*) = 2/4 → 1/2

*J*(*B*, *C*) = 1/4

*J*(*A*, *C*) = 1/4


### Answer: Baskets A and B are farthest apart, based on Jaccard distance.

**Question 5.2:** Assume that this small grocery store ***only*** sells 4 products: {Pudding, Jam, Salsa, Chips}. Bearing in mind that the Jaccard distance is well suited to asymmetric binary data, is it a good idea to use it in the context described here? Why or why not?

### Answer: Yes, it is a good idea to use Jaccard distance in the context described here. Given that there are only four items sold at this grocery store, N~00~ is not as important as N~11~. In other words, two people buying the same product is more informative to management than neither person buying the same product. In fact, the three item list provided includes no instance of N~00~.

#

**Question 6:** Consider the following "customer.csv" dataset.

Answer the following two sub-questions:

Question 6.1: Use the **Euclidean** distance metric *directly* on the given data to determine which customer (among B, C, and D) is most similar to customer A.

```{r}
customer = read.csv("customer.csv")

euclidian_distance_matrix = dist(customer[,2:4], method = "euclidean")

as.matrix(euclidian_distance_matrix)
```

### Answer: Based on the Euclidian distance metric directly on the given data, customer D is most similar to customer A.

Question 6.2: **Normalize** the customer data using **min-max normalization**. Then, use the Euclidean distance metric on the normalized data to determine which customer (among B, C, and D) is most similar to customer A.

```{r}
normalize = function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
  }

customer_normalized = customer %>%
  mutate_at(c(2:4), normalize)

euclidian_distance_matrix = dist(customer_normalized[, 2:4], method = "euclidean")

as.matrix(euclidian_distance_matrix)
```

### Answer: Based on the Euclidian distance metric using min-max normalization on the given data, customer C is most similar to customer A.

#

# Part III. Data Manipulation in R

**Question 7:**

1. Import the “hills.csv” dataset.

```{r, warning = FALSE, message = FALSE}
hills = read.csv("hills.csv")
```

2. Use the dplyr function to arrange data based on “Time” and write down the race with the longest time.

```{r}
longest_time = hills %>% arrange(Time)

longest_time
```

### Answer: The race with the longest time is BensofJura.

3. Use dplyr function to get a subset of data with “Climb” less than 1000. Report the number of records in the subset.

```{r}
data_climb = hills %>% filter(Climb < 1000)

nrow(data_climb)
```

### Answer: The number of records in the subset is 17.

4. Create a histogram of the "Time" variable.

```{r}
hist(hills$Time)
```

5. Create a scatterplot of the "Distance" and "Time" variables.

```{r}
plot(hills$Distance, hills$Time)
```

6. Create a boxplot of the "Distance" variable, grouped based on whether the "Time" for the race is $\geq$ 39.75 (or not). You should end up with two boxplots, side by side, in one chart.

```{r}
hills = hills %>% mutate(Time_gte_39.75 = ifelse(hills$Time >= 39.75, "Time >= 39.75", "Time < 39.75"))

boxplot(Distance ~ Time_gte_39.75, data = hills)
```

Then, interpret the components of either one of the two boxes. 

In particular, interpret the following components:

(a) What does the thick black horizontal line represent?

### Answer: The thick black horizontal line represents the median.

(b) What does the vertical height of the rectangle box in the chart represent?

### Answer: The vertical height of the rectangle box in the chart represents the interquartile range (IQR) between the first (Q1) and third (Q3) quartiles where the middle fifty percent of the data is located.

(c) What do the two horizontal lines above and below the rectangle box (outside the box) represent?

### Answer: The two horizontal lines above and below the rectangle box represent the maximum and minimum for the normal range of data (adjusted for outliers).

#

# Part IV. Mine Association Rules from Data in R

**Question 8:**

1. Import the “groceries.csv” dataset as transaction data.

```{r, warning = FALSE, message = FALSE}
library(arules)

groceries = read.transactions("groceries.csv",
                           format = "basket",
                           sep = ",",
                           rm.duplicates = TRUE)
```

2. Find out the total number of unique items in this data.

```{r}
nrow(itemInfo(groceries))
```

### Answer: There are 169 unique items in this data.

3. Find all association rules with a minsupp of 0.05 and a minconf of 0.05.

```{r}
rules = apriori(groceries,
                parameter = list(supp = 0.05, conf = 0.05))
```

4. Inspect the rules and report the total number of rules found.

```{r}
inspect(rules)
```

### Answer: 34 association rules with a minsupp of 0.05 and a minconf of 0.05 were found.

5. Get a subset of rules whose LHS contain “whole milk”. Report the subset of rules.

```{r}
inspect(subset(rules, lhs %pin% "whole milk"))
```

6. Report the rule with the highest lift (among the subset of rules in the last question). Interpret that rule. Also, interpret the lift ratio. Discuss what actions can be taken based on this rule.


### Answer: The rule {whole milk} → {yogurt} has the highest lift, with a value of 1.571735.

Interpret this rule with the highest lift.

### Answer: {whole milk} → {yogurt}, this rule with the highest lift, can be interpreted as customers who buy milk are likely to buy yogurt.

Interpret the lift ratio.

### Answer: The lift of 1.571735 us that customers who buy whole milk are 0.571735x (57.1735%) more likely to buy yogurt than customers in general.

As a store manager, what action could you take after you know this rule?

### Answer: As a store manager, knowing that customers who buy whole milk are likely to buy yogurt, I could situate whole milk closer to yogurt in my store, as a way to make shopping more convenient for my customers. Given the high likelihood that these two itemsets co-occur more than than pure chance, the location of these two items in my store should not matter. As a result, I could also place whole milk further away from yogurt in my store. I could even bundle these two items together as part of a breakfast promotion, given that these two items are typically consumed in the mornings. Finally, I could lower the price of whole milk and raise the price of yogurt, as I know there is a likelihood that whole milk buyers will still buy yogurt, despite the price increase.


# Part V. Clustering Analysis in R

**Question 9:**

The "utility.csv" dataset contains information on 22 electricity power utilities. The first two columns are the name and ID of each utility, and the other columns (x1 - x8) are numeric characteristics of each utility. 

1. Are all variables necessary for the purpose of clustering analysis? If so, explain why. If not, explain and select only the ones that are necessary.

### Answer: No, not all variables are necessary for the purpose of clustering analysis. The name and ID variables are not necessary.

2. Is normalization necessary? Why or why not? If it is necessary, conduct normalization on selected variables.

### Answer: Yes, there is a need to normalize the data. Column x6, compared to all other columns, varies greatly in range of values (3,300 - 17,441). If the data is not normalized, x6 would cause distortion in subsequent calculations.

```{r}
utility = read.csv("utility.csv")

normalize = function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
  }

utility_normalized = utility %>%
  mutate_at(c(3:10), normalize)
```

3. Get the distance matrix using **Manhattan** distance.

```{r}
distance_matrix = dist(utility_normalized[, 3:10], method = "manhattan")

as.matrix(distance_matrix)
```

4. Apply hierarchical clustering using Ward’s method.

```{r}
hierarchical = hclust(distance_matrix, method = "ward.D")
```

5. Plot the dendrogram and report the number of clusters that you see fit.

```{r}
plot(hierarchical, labels = utility_normalized$utility_name)
```

### Answer: I think that four clusters are appropriate.

6. Mark the clusters on the dendrogram, based on your answer to the last question.

```{r}
plot(hierarchical, labels = utility_normalized$utility_name)
rect.hclust(hierarchical, k = 4)
```

7. Apply K-Means clustering, using the number of clusters from the last two questions.

```{r}
kcluster = kmeans(utility_normalized[, 3:10], centers = 4)
```

8. Report the cluster centroids and interpret each cluster. Note that you do not have to differentiate the clusters on every single variable. Rather, try to describe each cluster by its most distinguishable characteristics. It is useful to know the meaning of each variable. Here they are:

• x1:  Fixed - charge covering ration (income/debt)

• x2:  Rate of return of capital

• x3:  Cost per KW capacity in place

• x4:  Annual Load Factor

• x5:  Peak KWH demand growth from 1974 to 1975

• x6:  Sales (KWH use per year)

• x7:  Percent Nuclear

• x8:  Total fuel costs (cents per KWH)

```{r}
kcluster$centers
```

### Answer:
### Cluster 1: Has high total fuel costs (cents per KWH) (x8) and annual load factor (x4).
### Cluster 2: Has high percent nuclear (x7), with every other variable being low-to-moderate.
### Cluster 3: Has high cost per KW capacity in place (x3), peak KWH demand growth from 1974 to 1975 (x5), and sales (KWH use per year)(x6), with no percent nuclear (x7).
### Cluster 4: High fixed - charge covering ration (income/debt) (x1) and rate of return of capital (x2).

9. Find the most natural number of clusters by plotting the SSE curve and explain *how* you found the cluster number.

```{r}
SSE_curve <- c()
for (n in 1:10) {
  kcluster = kmeans(utility_normalized[, 3:10], n)
  sse = kcluster$tot.withinss
  SSE_curve[n] = sse
  }

plot(1:10, SSE_curve, type = "b")
```

### Answer: The most natural number of clusters in this data is four. You find the most natural number of clusters by looking for the "elbow" of the SSE plot. The elbow is the point where the SSE drops greatly before but very little after. Looking at the plot, the SSE drops greatly between one and four clusters and very little between four and ten clusters, which is why I decided four to be the most natural number of clusters in this data.